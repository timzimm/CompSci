{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "015b173d-b529-484c-9b25-2774211cbeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats.qmc import LatinHypercube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1830e22-3af1-4b75-b7b3-ab2d5a32a35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejection_sampling(pdf, n=1000, xmin=0, xmax=.98):\n",
    "    x=torch.linspace(xmin,xmax,1000)  \n",
    "    y=pdf(x)  \n",
    "    pmin=0.  \n",
    "    pmax=y.max()  \n",
    "   \n",
    "    naccept=0  \n",
    "   \n",
    "    ran=[] # output list of random numbers  \n",
    "    while naccept<n:  \n",
    "        x = (xmax - xmin) * torch.rand(1) + xmin  \n",
    "        y = (pmax - pmin) * torch.rand(1) + pmin   \n",
    "        \n",
    "        if y<pdf(x):  \n",
    "            ran.append(x)  \n",
    "            naccept=naccept+1  \n",
    "    \n",
    "    return np.asarray(ran)  \n",
    "\n",
    "def generate_random_symmetric_matrix(dim):\n",
    "    M_random = torch.rand((dim,dim)) - 0.5\n",
    "    return 0.5 * (M_random.T + M_random)\n",
    "\n",
    "def eigenvalue_of_eigenvector(x):\n",
    "    return ((x @ A @ x.T) / (x @ x.T)).item()\n",
    "\n",
    "def derivative(y, x):\n",
    "    dim = y.shape[-1]\n",
    "    dydx = torch.zeros_like(y, dtype=y.dtype)\n",
    "    for i in range(dim):\n",
    "        yi = y[:,i].sum()\n",
    "        dydx[:,i] = torch.autograd.grad(yi, x, create_graph=True, retain_graph=True, allow_unused=True)[0].squeeze()\n",
    "    return dydx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ce6233c-bfbc-4a43-a267-b1792329d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EigenPiNN(nn.Module):\n",
    "    def __init__(self, dim, hidden_layers):\n",
    "        super().__init__()\n",
    "\n",
    "        # Hidden Layers are assumed to be fully connected acyclic ReLu nodes with hidden_layer nodes each.\n",
    "        # The ODE leading to an eigenvector fix point is first order in \"time\"\n",
    "        # It is thus possible to use a ReLU activation. Only second derivatives would be problematic \n",
    "        # (as they vanish)\n",
    "        prev_nodes_per_layer = 1;\n",
    "        hidden = []\n",
    "        for nodes_per_layer in hidden_layers:\n",
    "            hidden += [nn.Linear(prev_nodes_per_layer, nodes_per_layer), nn.ReLU()]\n",
    "            prev_nodes_per_layer = nodes_per_layer\n",
    "        \n",
    "        self.ff_relu_graph = nn.Sequential(*hidden)\n",
    "            \n",
    "        # Output Layer is assumed to be linear\n",
    "        self.output = nn.Linear(prev_nodes_per_layer, dim)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return self.output(self.ff_relu_graph(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "819b1039-0c76-44c8-b1d3-588cb31d301d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, A):\n",
    "    first_term = (x[:,None,:] @ (x[:,None,:] @ x[:,:,None] * A)).squeeze()\n",
    "    second_term = (x[:,None,:] @ A @ x[:,:,None] @ x[:,None,:]).squeeze()\n",
    "    return first_term - second_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40a41615-f81f-4546-9153-8ed305476aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eigendecomposition(A, verbose=True):\n",
    "    \n",
    "    # Coordinate Transformations\n",
    "    def dtau_dt_inv_tan(tau):\n",
    "        \"\"\"\n",
    "        Assuming t = tan(pi/2 * tau) this is (dtau/dt)^(-1)\n",
    "        \"\"\"\n",
    "        return torch.pi/(2 * torch.cos(torch.pi/2 * tau)**2)\n",
    "\n",
    "    def dtau_dt_inv_arctanh(tau):\n",
    "        \"\"\"\n",
    "        Assuming t = arctanh(tau) this is (dtau/dt)^(-1)\n",
    "        \"\"\"\n",
    "        return 1.0/(1.0 - tau**2)\n",
    "    \n",
    "    dim = A.shape[0]\n",
    "    \n",
    "    #Initial Condition\n",
    "    y_0 = torch.rand(dim).reshape(1,dim)\n",
    "    y_0 /= torch.sqrt(y_0 @ y_0.T).item()\n",
    "    y_0 = y_0.reshape(1,-1)\n",
    "    \n",
    "    # Hyperparameters for model and learning\n",
    "    hidden_layers = [200,200]\n",
    "    batchsize= 2**9\n",
    "    number_of_minibatches=1\n",
    "    epochs = 2000\n",
    "    \n",
    "    # Hyperparameters for the dataset\n",
    "    fixed_dataset = False\n",
    "    kind = \"lhs\"\n",
    "    sampler = LatinHypercube(1)\n",
    "    \n",
    "    if fixed_dataset:\n",
    "        if kind == \"rejection\":\n",
    "            samples = rejection_sampling(dtau_dt_inv_tan, n=batchsize * number_of_minibatches, \n",
    "                                         xmin=0.01, xmax=0.99).reshape(-1,1)\n",
    "        elif kind == \"linspace\":\n",
    "            samples = np.linspace(0.01, 0.99, batchsize * number_of_minibatches).reshape(-1,1)\n",
    "        elif kind == \"lhs\":\n",
    "            samples = .98 * sampler.random(n=batchsize) + .01\n",
    "        \n",
    "    net = EigenPiNN(dim, hidden_layers)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(net.parameters())\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "    if verbose:\n",
    "        mean_loss = 0.0\n",
    "        report_after_e_epochs = min(epochs, 100)\n",
    "        eigenvalue_evolution = []\n",
    "        print(f'{\"Hyperparameters\":-^60}')\n",
    "        print(f'{\"Network Layers/Nodes:\":<40}{hidden_layers}')\n",
    "        print(f'{\"Number of collocation points per batch:\":<40}{batchsize:>20}')\n",
    "        print(f'{\"Number of minbatches:\":<40}{number_of_minibatches:>20}')\n",
    "        print(f'{\"Points per Iteration:\":<40}{batchsize * number_of_minibatches:>20}')\n",
    "        print(f'{\"Points fixed?:\":<40}{fixed_dataset:>20}')\n",
    "        print(f'{\"Iterations:\":<40}{epochs:>20}')\n",
    "        print(f'{\"Total number of collocation points\":<40}{epochs * batchsize * number_of_minibatches:>20}')\n",
    "        print(f'{\"\":-^60}')\n",
    "        print(f'{\"Epoch\":^30}|{\"Mean Loss\":^30}')\n",
    "        \n",
    "    for e in range(1, epochs+1):\n",
    "        for minibatch in range(number_of_minibatches):\n",
    "            \n",
    "            # Interior\n",
    "            if fixed_dataset:\n",
    "                tau_interior = torch.tensor(samples[minibatch*batchsize:(minibatch+1)*batchsize], \n",
    "                                            dtype=torch.float32, \n",
    "                                            requires_grad=True)\n",
    "            else:\n",
    "                if kind == \"rejection\":\n",
    "                    tau_interior = torch.tensor(rejection_sampling(dtau_dt_inv_tan, \n",
    "                                                                   n=batchsize * number_of_minibatches, \n",
    "                                                                   xmin=0.01, xmax=0.99),\n",
    "                                                dtype=torch.float32, \n",
    "                                                requires_grad=True)\n",
    "                elif kind == \"linspace\":\n",
    "                    tau_interior = torch.tensor(np.linspace(0.01, 0.99, batchsize), \n",
    "                                                dtype=torch.float32, \n",
    "                                                requires_grad=True)\n",
    "                elif kind == \"lhs\":\n",
    "                    tau_interior = torch.tensor(.98 * sampler.random(n=batchsize) + .01, \n",
    "                                                dtype=torch.float32, \n",
    "                                                requires_grad=True)\n",
    "            tau_interior.reshape(-1,1)\n",
    "                \n",
    "            # Forward pass of all interior (collocation) points tau in [0.01, 0.99)\n",
    "            y_interior = y_0 + tau_interior * net(tau_interior)\n",
    "            \n",
    "            dydtau = derivative(y_interior, tau_interior)\n",
    "            \n",
    "            dtau_dt_inv = dtau_dt_inv_tan(tau_interior)\n",
    "            #dtau_dt_inv = dtau_dt_inv_arctanh(tau_interior)\n",
    "            \n",
    "            pred_interior, target_interior = dydtau - dtau_dt_inv * f(y_interior, A), torch.zeros_like(dydtau)\n",
    "            mse_interior = loss_fn(pred_interior, target_interior)\n",
    "            \n",
    "            # Boundary\n",
    "            tau_boundaries = torch.tensor([1.]).reshape(1,1)\n",
    "            \n",
    "            # Forward pass of tau=1\n",
    "            y_boundary = y_0 + tau_boundaries * net(tau_boundaries)\n",
    "    \n",
    "            # See Theorem 1\n",
    "            fy_1_model = f(y_boundary[-1,:].reshape(1,-1), A).reshape(1,-1)\n",
    "            y_1 = torch.zeros((1,dim), dtype=A.dtype)\n",
    "            \n",
    "            pred_boundary, target_boundary = fy_1_model, y_1\n",
    "            mse_boundary = loss_fn(pred_boundary, target_boundary)\n",
    "            \n",
    "            #Total Loss\n",
    "            loss = mse_interior + mse_boundary\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if verbose:\n",
    "                mean_loss = mean_loss + loss.item()/report_after_e_epochs\n",
    "            \n",
    "        with torch.autograd.no_grad():\n",
    "            if verbose and e % report_after_e_epochs == 0 and e > 1:\n",
    "                print(f'{e:^30}|{mean_loss:>29.6e}')\n",
    "                mean_loss = 0.0\n",
    "                tau_boundaries = torch.tensor([1.]).reshape(1,1)\n",
    "                y_boundary = net(tau_boundaries)\n",
    "                eigenvalue_evolution.append(eigenvalue_of_eigenvector(y_boundary[-1,:].reshape(1,-1)))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'{\"Iteration limit reached\":-^60}')\n",
    "        return net, eigenvalue_evolution\n",
    "    else:\n",
    "        return y_1_model, eigenvalue_of_eigenvector(y_boundary[-1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "011ebe1c-0f02-4965-9f5c-a76734269481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = tensor([[ 0.2500,  0.5000,  0.7500,  1.0000,  1.2500],\n",
      "        [ 0.5000, -0.2500, -0.5000, -0.7500, -1.0000],\n",
      "        [ 0.7500, -0.5000,  0.2500,  0.2500,  0.2500],\n",
      "        [ 1.0000, -0.7500,  0.2500,  0.0000,  0.0000],\n",
      "        [ 1.2500, -1.0000,  0.2500,  0.0000,  0.0000]])\n",
      "max{ Sp(A) } = 2.2536652088165283\n",
      "[[-0.5393615   0.28132373  0.789159    0.01174785  0.08388205]\n",
      " [-0.17392091 -0.40364477 -0.05408083 -0.4034128   0.80072206]\n",
      " [-0.3810284  -0.40079176 -0.04650102 -0.5908321  -0.5856094 ]\n",
      " [-0.49519905 -0.49759743 -0.16907792  0.69146746 -0.02144955]\n",
      " [ 0.53706235 -0.59134644  0.5861345   0.09955014 -0.0917036 ]]\n"
     ]
    }
   ],
   "source": [
    "# Eigenvalue Problem\n",
    "dim = 6\n",
    "#A = generate_random_symmetric_matrix(dim)\n",
    "A = torch.tensor(1/4 * np.array([[1,2,3,4,5],[2,-1,-2,-3,-4],[3,-2,1,1,1],[4,-3,1,0,0],[5,-4,1,0,0]]), dtype=torch.float32)\n",
    "print(f\"A = {A}\")\n",
    "\n",
    "#Ground Truth\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "# Sort in descending order\n",
    "permutation = eigenvalues.argsort()[::-1]\n",
    "eigenvalues = eigenvalues[permutation]\n",
    "eigenvectors = eigenvectors[permutation]\n",
    "print(f\"max{{ Sp(A) }} = {np.max(eigenvalues)}\")\n",
    "print(eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e45b191b-2e9f-48dc-a605-42f8a5c86234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------Hyperparameters-----------------------\n",
      "Network Layers/Nodes:                   [200, 200]\n",
      "Number of collocation points per batch:                  512\n",
      "Number of minbatches:                                      1\n",
      "Points per Iteration:                                    512\n",
      "Points fixed?:                                             0\n",
      "Iterations:                                             2000\n",
      "Total number of collocation points                   1024000\n",
      "------------------------------------------------------------\n",
      "            Epoch             |          Mean Loss           \n",
      "             100              |                 1.103423e+03\n",
      "             200              |                 4.637231e-01\n",
      "             300              |                 1.650380e-01\n",
      "             400              |                 1.356591e-01\n",
      "             500              |                 1.283672e-01\n",
      "             600              |                 1.249227e-01\n",
      "             700              |                 1.222047e-01\n",
      "             800              |                 1.084873e-01\n",
      "             900              |                 1.122977e-01\n",
      "             1000             |                 1.829857e-01\n",
      "             1100             |                 1.149552e-01\n",
      "             1200             |                 1.066178e-01\n",
      "             1300             |                 9.069830e-02\n",
      "             1400             |                 1.456086e-01\n",
      "             1500             |                 8.763375e-02\n",
      "             1600             |                 7.895198e-02\n",
      "             1700             |                 7.028466e-02\n",
      "             1800             |                 1.180852e-01\n",
      "             1900             |                 6.988539e-02\n",
      "             2000             |                 8.696542e-02\n",
      "------------------Iteration limit reached-------------------\n",
      "max{ Sp(A) } = -0.47311779856681824\n"
     ]
    }
   ],
   "source": [
    "converged_network , eigenvalue_evolution = eigendecomposition(A, verbose=True)\n",
    "print(f\"max{{ Sp(A) }} = {eigenvalue_evolution[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecf9b477-5b60-4497-a010-86d61caee24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAASp0lEQVR4nO3dbWxkV33H8d9/xjP2eD1+2Ni73qd0A2GDQlWS1EqBtAlboAoRIgWpVVDVRgVphVQkIrUSqSIh3lJUXpSioq2IoFUEqIKUqA2FQFdZFTUpTrQJSTa72QRQNvbuerO79vh5PPPvi3ttj8dj79hzPeOz+/1Io7kP595zfHz985kz1x5zdwEAwpVqdQMAAI0hyAEgcAQ5AASOIAeAwBHkABC4tlZU2t/f7wcPHmxF1QAQrOeee+6iuw9Ub29JkB88eFDDw8OtqBoAgmVmv6m1nakVAAgcQQ4AgSPIASBwBDkABI4gB4DAEeQAEDiCHAAC15L7yAuFgo4dO9aKqgHgmsOIHAAC15IReT6f1+HDh1tRNQBccxiRA0DgCHIACBxBDgCBI8gBIHAEOQAEjiAHgMA1HORmdsDMjpnZSTN72cw+n0TDAAD1SeI+8gVJf+3uz5tZXtJzZvaUu7+SwLkBAFfR8Ijc3Ufd/fl4uSDppKR9jZ4XAFCfROfIzeygpNslPVtj3xEzGzaz4bGxsSSrBYDrWmJBbmZdkr4v6SF3n6je7+5H3X3I3YcGBlZ9CDQAYJMSCXIzyygK8cfc/QdJnBMAUJ8k7loxSd+UdNLdv9p4kwAAG5HEiPwuSX8u6Q/N7ET8uC+B8wIA6tDw7Yfu/j+SLIG2AAA2gb/sBIDAEeQAEDiCHAACR5ADQOAIcgAIHEEOAIEjyAEgcAQ5AASOIAeAwBHkABA4ghwAAkeQA0DgCHIACBxBDgCBI8gBIHAEOQAEjiAHgMAR5AAQOIIcAAJHkANA4AhyAAgcQQ4AgSPIASBwBDkABI4gB4DAEeQAEDiCHAACR5ADQOAIcgAIHEEOAIEjyAEgcAQ5AASOIAeAwBHkABC4RILczB41swtm9lIS5wMA1C+pEfm3JN2b0LkAABuQSJC7+3FJl5I4FwBgY5o2R25mR8xs2MyGx8bGmlUtAFzzmhbk7n7U3YfcfWhgYKBZ1QLANY+7VgAgcAQ5AAQuqdsPvyPpfyXdYmZnzewzSZwXAHB1bUmcxN0/lcR5AAAbx9QKAASOIAeAwBHkABA4ghwAAkeQA0DgCHIACBxBDgCBI8gBIHAEOQAEjiAHgMAR5AAQOIIcAAJHkANA4AhyAAgcQQ4AgSPIASBwBDkABI4gB4DAJfJRbxtVKBR07NixVlQNANccRuQAELiWjMjz+bwOHz7ciqoB4JrDiBwAAkeQA0DgCHIACBxBDgCBI8gBIHAEOQAEjiAHgMAR5AAQOIIcAAJHkANA4AhyAAgcQQ4AgSPIASBwiQS5md1rZqfM7IyZPZzEOQEA9Wn439iaWVrS1yV9RNJZSb8wsyfc/ZW1juGDJQAgOUmMyO+UdMbd33D3eUnflXR/AucFANQhiQ+W2CfpzYr1s5J+r7qQmR2RdESSbrzxRj5YAgASksSI3Gps81Ub3I+6+5C7Dw0MDCRQLQBASmZEflbSgYr1/ZJG1jvglVde0W233ZZA1QCAJEbkv5D0LjO7ycyykh6Q9EQC5wUA1KHhEbm7L5jZ5yT9WFJa0qPu/vJ6x9x6660aHh5utGoAuK6Y1ZrJTmZqRe7+pKQnkzgXAGBj+MtOAAgcQQ4AgSPIASBwBDkABI4gB4DAEeQAEDiCHAACR5ADQOAIcgAIHEEOAIEjyAEgcAQ5AASOIAeAwCXy3w+bZWK2qM5MWm1pfv8A1yp319tT83rr8oxGrszorfgxcmVGb0/O64aurPb05LS3t0N7enLa09OhPb057c63J5IN7q7L00WNjs/o/MSsRsdndW48er44OaeySymTUmZKWfSvZZfXTVa1zSrKpsz0l3fdpFsG8wn01LKggvxrP3tN3/2/N3XXzf26+9CA7j7Ur/19na1uFoANmC2WdG58ViNXZnQ2DuiRpbCe1VtXZjS/UF5xTGc2rX29Oe3ckdUbY1P6+Zm3NTm3sKJMyqRd+Q7t6e2Iwj0O+b29uaX1G7qyujQ1vxTM58ZnNDoRBfW58Vmdi4O7uv7Fcw/k25VOmdxdZZfK8XO0vrzNl7Ytr5fjMh+/ba+k6zjI7zm0S4XZBR0/Pab/evmcJOmdAzt0z6FduvtQv973jhvUkUm3uJVA8txdo3H47e6Ogmq7vTKdnl/QhYk5jU3ORc+F2eXlyTmNFeZ0fmJOFyfnVh27K9+uvb053bq3Wx+5dbf29nRoX1+n9vZ2aF9vTj25zKoPVZiYLWr0yqxGxmc0eiUK5ZHxWY2Oz+jV0YL++9ULmi2WV9VVLZtOaXdPu/Z05/Te/b269z0dS308GP8C6O/Kbrv+rmTuqz4necsNDQ15I58Q5O46c2FST58e09Onx/Tsry5pfqGs9raU7rxpp+45NKB7Dg3o5l1da36iBrBdjc8Udfp8Qa+eK+jUuQmdOhctF2aXR6DplGmwu0MHdua0v69TB/o6tb8vpwM7o+fd3R1Kpxq79ucXyhqfKWp8Zl5Xpou6PF3U5el5jRXmVjwuFGY1VpjT1Hxp1TnSKVN/V3ZpNLsY2NGjQ/t7O7W7p13tbckPwNxd4zNFjVyJwn1kfFaXJue1syurPd2LId2hnTuyweSEmT3n7kOrtocY5NVm5kt69ldv6/jpi3r69AW9PjYlSdrb06G741D/wM396sllEqsTtbm7ZouLAbD6MT23oFRq9bzhynnGaI4xnaqcg4yWs22ppdHSrnyHsm3bd5R0NfMLZb0+NrkU1IuhPTI+u1Qm39Gmdw/mdctgXrcMdmt/b04XCrN689KMzl6e1puXo+fzEytHuZm0aV9vHPJx2O/vy2kg366puZKuTEfhfGVm8bm4vG06+l5VT11Uyre3aaC7XQNd7XFAR0G9GNaLz32dWaUa/IWCZdd0kFc7e3lax09f1PHTY/r5mYsqzC0onTLdfqBXv3uwTzfsyKo3l1VPZ0a9uYz6dmTVm8uopzOzJSODrVQuu+ZLZRVLZRVLrmKprPmFqvVSWcWFeL1cVrlcOXfnKpWX5+/K7iqvWI+X42OKpbVDemKmqImZBc2Xrv5yNglmUn9Xezz/Gb0EXhxlDXZHc6O7urdmtLeWYqmsqbkFFWYXNDkXP2YXVIifL03N6dT5SZ06N6E3xqa0UI5+/jJp0zsHuuLQ7l4K7z09HXWNFmeLJY1cmVkK9sqgf+vytC5Oztc8Lp2ypWu/r3P556A3l1VvZ0Z9nRn1xNt74zL9Xe3KZcP6OblWXFdBXqlYKuvEm1f09KloGubk6MTSD08tuUxavZ0Z9eTiC7szE69Hyzuyabm0IgwlrXqjY0VYavnNkFJZK8J2fjFwV6wv7vcV24oLVeslV2mdr2WrpEzqzkV9tPjo7sis2ra0L9e2tNyZbVvqr8o3gMpVbw6t3B/1p7s0UyzFdxLMrLibYHG9cvphUX9XVoM9HRrszmkgn1WqIhgrM9JUe3u0L1Jy19RcKQ7qoqbmSppcCu5iXXOy+/tyK0bZ7x7M66b+Hcps4Rzs9PyC3ro8o7HCnPIdmaXruqu9LZhpBVzHQV7N3TU1X9LlqXmNzxRXvLwcr3h5eXl6eW5w8WVnsbT5vqq8PSnbllImbcqkU8q2pZRNp5RJp5Rpi7Zl0ovbVpdpS9vKY+LjVqynF+uoWE+nlGlLKZ0ypeNpi8XpjcopjPTiNEZqub2Vy5m0aUe2bdu+XJ6cW4juRBivvG1sOfSjkWn0fay89Cu/s9U/E5VrJqmro01d7Rnl29vi5Yrn9uX1yv35+JjuXNvSLzNgo9YK8uvuijKzpR+2Axs4zt01Uyxpaq60IpRlK0N69dyvGPE0UVd7m27eldfNu5K9vQvYzq67IN8sM1NnltEUgO0n3Lf8AQCSCHIACB5BDgCBI8gBIHAEOQAEjiAHgMAR5AAQOIIcAAJHkANA4AhyAAgcQQ4AgWsoyM3sT8zsZTMrm9mq/8gFANh6jY7IX5L0SUnHE2gLAGATGvpXfu5+UuLftAJAKzVtjtzMjpjZsJkNj42NNataALjmXXVEbmY/lTRYY9cj7v7Deity96OSjkrRJwTV3UIAwLquGuTu/uFmNAQAsDncfggAgWv09sNPmNlZSe+X9J9m9uNkmgUAqFejd608LunxhNoCANgEplYAIHAEOQAEjiAHgMAR5AAQOIIcAAJHkANA4AhyAAgcQQ4AgSPIASBwBDkABI4gB4DAEeQAEDiCHAACR5ADQOAIcgAIHEEOAIEjyAEgcAQ5AASOIAeAwBHkABA4ghwAAkeQA0DgCHIACBxBDgCBI8gBIHAEOQAEjiAHgMAR5AAQOIIcAAJHkANA4AhyAAgcQQ4AgSPIASBwDQW5mX3FzF41sxfN7HEz602oXQCAOjU6In9K0m+7++9IOi3pbxtvEgBgIxoKcnf/ibsvxKvPSNrfeJMAABuR5Bz5pyX9KMHzAQDq0Ha1Amb2U0mDNXY94u4/jMs8ImlB0mPrnOeIpCOSdOONN26qsQCA1a4a5O7+4fX2m9mDkj4m6UPu7uuc56iko5I0NDS0ZjkAwMZcNcjXY2b3SvqCpHvcfTqZJgEANqLROfJ/lJSX9JSZnTCzbyTQJgDABjQ0Inf3m5NqCABgc/jLTgAIHEEOAIEjyAEgcAQ5AASOIAeAwBHkABA4ghwAAkeQA0DgCHIACFxDf9m5WYVCQceOHWtF1QBwzWFEDgCBa8mIPJ/P6/Dhw62oGgCuOYzIASBwBDkABI4gB4DAEeQAEDiCHAACR5ADQOAIcgAIHEEOAIEzd29+pWZjkn6zycP7JV1MsDlJo32NoX2NoX2N285t/C13H6je2JIgb4SZDbv7UKvbsRba1xja1xja17gQ2liNqRUACBxBDgCBCzHIj7a6AVdB+xpD+xpD+xoXQhtXCG6OHACwUogjcgBABYIcAAK3bYPczO41s1NmdsbMHq6x38zsH+L9L5rZHU1s2wEzO2ZmJ83sZTP7fI0yHzSzcTM7ET++2Kz2xfX/2sx+Gdc9XGN/K/vvlop+OWFmE2b2UFWZpvafmT1qZhfM7KWKbTvN7Ckzey1+7lvj2HWv1S1s31fM7NX4+/e4mfWucey618IWtu9LZvZWxffwvjWObVX/fa+ibb82sxNrHLvl/dcwd992D0lpSa9LeoekrKQXJN1aVeY+ST+SZJLeJ+nZJrZvj6Q74uW8pNM12vdBSf/Rwj78taT+dfa3rP9qfK/PKfpDh5b1n6S7Jd0h6aWKbX8n6eF4+WFJX16j/eteq1vYvj+S1BYvf7lW++q5FrawfV+S9Dd1fP9b0n9V+/9e0hdb1X+NPrbriPxOSWfc/Q13n5f0XUn3V5W5X9K/eOQZSb1mtqcZjXP3UXd/Pl4uSDopaV8z6k5Qy/qvyockve7um/1L30S4+3FJl6o23y/p2/HytyX9cY1D67lWt6R97v4Td1+IV5+RtD/peuu1Rv/Vo2X9t8jMTNKfSvpO0vU2y3YN8n2S3qxYP6vVQVlPmS1nZgcl3S7p2Rq7329mL5jZj8zsPc1tmVzST8zsOTM7UmP/tug/SQ9o7R+gVvafJO1291Ep+uUtaVeNMtulHz+t6BVWLVe7FrbS5+Kpn0fXmJraDv33B5LOu/tra+xvZf/VZbsGudXYVn2fZD1ltpSZdUn6vqSH3H2iavfziqYL3ivpa5L+vZltk3SXu98h6aOS/srM7q7avx36Lyvp45L+rcbuVvdfvbZDPz4iaUHSY2sUudq1sFX+SdI7Jd0maVTR9EW1lvefpE9p/dF4q/qvbts1yM9KOlCxvl/SyCbKbBkzyygK8cfc/QfV+919wt0n4+UnJWXMrL9Z7XP3kfj5gqTHFb2ErdTS/ot9VNLz7n6+eker+y92fnG6KX6+UKNMq6/DByV9TNKfeTyhW62Oa2FLuPt5dy+5e1nSP69Rb6v7r03SJyV9b60yreq/jdiuQf4LSe8ys5viUdsDkp6oKvOEpL+I7754n6TxxZfBWy2eU/umpJPu/tU1ygzG5WRmdyrq67eb1L4dZpZfXFb0pthLVcVa1n8V1hwJtbL/Kjwh6cF4+UFJP6xRpp5rdUuY2b2SviDp4+4+vUaZeq6FrWpf5Xsun1ij3pb1X+zDkl5197O1dray/zak1e+2rvVQdFfFaUXvaD8Sb/uspM/Gyybp6/H+X0oaamLbfl/Ry78XJZ2IH/dVte9zkl5W9C78M5I+0MT2vSOu94W4Dduq/+L6OxUFc0/Ftpb1n6JfKKOSiopGiZ+RdIOkn0l6LX7eGZfdK+nJ9a7VJrXvjKL55cVr8BvV7VvrWmhS+/41vrZeVBTOe7ZT/8Xbv7V4zVWUbXr/NfrgT/QBIHDbdWoFAFAnghwAAkeQA0DgCHIACBxBDgCBI8gBIHAEOQAE7v8BXuSgY5S8g64AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ev_nn = eigenvalue_evolution[-1]\n",
    "closest_true_ev = eigenvalues[(np.abs(eigenvalues - ev_nn)).argmin()]\n",
    "plt.plot(eigenvalue_evolution)\n",
    "for ev in eigenvalues:\n",
    "    plt.axhline(y=ev, color=\"darkgray\")\n",
    "    \n",
    "plt.axhline(y=closest_true_ev, color=\"k\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bfd040-a17c-471c-9281-ac7c90da6faa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
